static inline unsigned long __my_cpu_offset(void)							.\arch\arm64\include\asm\percpu.h
{
	unsigned long off;

	/*
	 * We want to allow caching the value, so avoid using volatile and
	 * instead use a fake stack read to hazard against barrier().
	 */
	asm(ALTERNATIVE("mrs %0, tpidr_el1",
			"mrs %0, tpidr_el2",
			ARM64_HAS_VIRT_HOST_EXTN)
		: "=r" (off) :
		"Q" (*(const unsigned long *)current_stack_pointer));

	return off;
}
#define __my_cpu_offset __my_cpu_offset()

#define RELOC_HIDE(ptr, off)                        \
({                                  \
    unsigned long __ptr;                        \
    __asm__ ("" : "=r"(__ptr) : "0"(ptr));              \
    (typeof(ptr)) (__ptr + (off));                  \						//主要这句	./include/linux/compiler-gcc.h +54
})

RELOC_HIDE((typeof(*(__p)) __kernel __force *)(__p), (__offset))

#define SHIFT_PERCPU_PTR(__p, __offset)					\
	RELOC_HIDE((typeof(*(__p)) __kernel __force *)(__p), (__offset))

#define arch_raw_cpu_ptr(ptr) SHIFT_PERCPU_PTR(ptr, __my_cpu_offset)

#define __verify_pcpu_ptr(ptr)						\
do {									\
	const void __percpu *__vpp_verify = (typeof((ptr) + 0))NULL;	\
	(void)__vpp_verify;						\
} while (0)

#define raw_cpu_ptr(ptr)						\
({									\
	__verify_pcpu_ptr(ptr);						\
	arch_raw_cpu_ptr(ptr);						\
})



#define raw_smp_processor_id() (*raw_cpu_ptr(&cpu_number))

DECLARE_PER_CPU_READ_MOSTLY(int, cpu_number);
	DECLARE_PER_CPU_SECTION(type, name, "..read_mostly")
	#define DECLARE_PER_CPU_SECTION(type, name, sec)
		extern __PCPU_ATTRS(sec) __typeof__(type) name
		extern __attribute__((section(.data..percpu..read_mostly))) __typeof__(type) name
		

#define PER_CPU_BASE_SECTION ".data..percpu"

#define __PCPU_ATTRS(sec)						\
	__percpu __attribute__((section(PER_CPU_BASE_SECTION sec)))	\
	__attribute__((section(PER_CPU_BASE_SECTION sec)))	\
	PER_CPU_ATTRIBUTES






















