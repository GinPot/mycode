多cpu读写一段参数(地址)时,或执行一段相同代码；

特点：
1、死等或者休眠死等；
2、临界区执行时间短, 不允许睡眠；
3、目前内核调用spin_lock时只是禁止该线程被抢占，所以中断handle中不能使用spin_lock；如果中断一定要访问共享资源，则需要使用spin_lock_bh/spin_lock_irq等api来禁止中断

场景分析：
	1、单CPU情况，进程A和进程B通过系统调用会访问共享资源R：
		A上锁后访问共享资源中发生了中断，中断唤醒了优先级更高的B，B先试过获取锁，由于A持有锁，导致了B永久的spin；
		
		解决办法：	A上锁的同时禁止本CPU的抢占(其实仍是禁止本CPU的中断，多线程调度依赖于定时器中断(这句话描述有问题，可以发生中断，但不允许其他线程抢占此线程))；(如果A和B运行在不同CPU上，B就可以正常spin等到A释放锁，在运行)
		也存在中断访问共享资源时的问题，所以需要关中断；但请参考特点3.
		
		
	2、多cpu情况，CPU0上的进程A，CPU1上的进程B，和中断都会访问共享资源R：
		CPU0上的A上锁进入共享资源时发生中断并调度到CPU1上执行，不同CPU上可以正常等待锁的释放；但中断调度到CPU0上时，由于A持有锁，导致了CPU0在中断中永久的spin，
		同时由于CPU0在中断中永久spin，没有机会运行A释放锁，导致CPU1上的B也会永久spin；
		
		解决办法：	和1一样，A上锁的同时禁止本CPU的中断；

bottom half：
	如果不是在中断handler中，而是在bottom half中访问共享资源，只需要禁止bottom half就可以							//不同的下半部访问共享资源的时候需要禁止软中断
	
中断上下文之间的竞争：
	1、同一种中断再单核或多核都不会并发执行，不需要spinlock保护；
	2、不同中断运行在一个核上时，因为中断handle会禁止中断，不需要使用spin lock保护共享资源；
	2.1、不同中断运行在不同核上时，需要普通的spinlock保护
	3、新内核所有中断handler都会关闭中断，使用spin lock不需要关闭中断配合；
	4、bottom half分为softirq和tasklet：
		4.1、同一种softirq会在不同CPU上并发执行，访问共享资源时需要spin lock，不用配合关CPU中断或bottom half；			//相同的下半部因为存在多CPU并发，需要spin lock
		4.2、同一种tasklet不会在多个CPU上并发；


历史阶段：
	1、最早自旋锁是无序竞争，不保证选申请的进程先获得锁；
		如在多核架构下，由于每个cpu都有自己的L1 cache，当获取锁时cache line状态是共享shared(S),CPU0释放锁后会将cache状态改为已修改Modified(M),smp_mb invalide其他忙等待CPU的L1 cache，使得释放锁的那个CPU可以更快的访问到L1 caceh，大大增加了下一次获取锁的机会；有在8核的环境中测试，某些线程被饥饿旋转了1 000 000次；
	2、第二阶段是入场券自旋锁，进程按照申请锁的顺序排队，先申请的进程先获的锁；
		还是存在cache的问题： 如多个cpu之间重复获取自旋锁，则会有多个cpu查询查询其值，锁所在cache line状态为shared，对该cache line数据进行修改将会导致其他CPU的cache未命中，如果锁不是独占cache line，还会影响同一cache line中的其他数据，因此锁争用会大大降低系统性能。
	3、第三阶段是MCS自旋锁；先申请全局的锁(主要next指针一直报错最后一个申请的锁地址，每个新锁先和全局交换next，得到前一个锁的信息，再修改前一个锁的next指向自己)，使用者申请per cpu内存的锁，在自己的锁上自旋；释放锁时会通过next修改下一个锁的per cpu值，达到空间换时间的效果
	  

	在armv8时提供了wfe指令用于自旋锁在未获得锁时，可以让CPU进入低功耗；同时用原子操作相关的指令去操作自旋锁的操作，在store操作时，操作的地址被标记为exclusive，那么global monitor的状态从exclusive access变成open access，同时会触发一个事件，唤醒其他wfe中的CPU(可以省略一个SEV的唤醒指令)，cpu继续判断是否可以获取锁...
		再细节：
				在cpu0调用unlock之前，各个状态机的状态如下：
				1、cpu0上针对spink lockmemory的状态机是open access状态。当然，这个状态和具体实现相关，不过，在这个场景中，没有人关注它的状态。
				2、cpu1上针对spink lockmemory的状态机是exclusive access状态，这时候cpu1处于wfe
				3、cpu2状态机和cpu1相同
				4、cpu3状态机和cpu1相同
				
				
				cpu0执行了spin_unlock操作，各个状态机的迁移情况如下：
				1、cpu0上针对spink lockmemory的状态机是怎样的呢？有人在乎吗？需要知道它的状态吗？当然不需要。
				2、cpu1的状态迁移：这时候实际上产生的事件是有其他cpu（指cpu0）执行了针对marked address（指共享的spin lock memory地址）的store操作，在状态图上对应Store(Marked_address,!n)事件，因此，该状态机迁移到open access状态，这时候cpu1的Event register会被写入event，就好象生成一个event，将cpu1唤醒
				3、cpu2类似cpu1
				4、cpu3类似cpu1
	
		依赖WFE：
			相同点： WFI和WFE都会是CPU进入standby模式：保持供电，关闭clock；
			不同点： WFI执行后，CPU会立即进入standby state，直到有WFI wakeup events发生；
					WFE执行后，会根据Event register(一个单bit寄存器，每个PE一个)的状态：如该寄存器为1，则会把它清零，然后完成执行；如果是0，WFE才会进入standby state，直到有WFE wakeup events发生；
				
				WFI和WFE的wakeup event可以使任何的IRQ、FIQ中断等；最大不同是WFE可以被任何PE上执行的SEV指令唤醒；
				SEV指令就是用来改变Event register的指令，有两个： SEV会修改所有PE的寄存器； SEVL只修改本PE的寄存器值。


===============================================================================================================================================================

入场券自旋锁：
static inline void arch_spin_lock(arch_spinlock_t *lock)
{
    unsigned int tmp;
    arch_spinlock_t lockval, newval;

    asm volatile(
    /* Atomically increment the next ticket. */
"    prfm    pstl1strm, %3\n"
"1:    ldaxr    %w0, %3\n"－－－－－（A）－－－－－－－－－－－lockval = lock
"    add    %w1, %w0, %w5\n"－－－－－－－－－－－－－newval ＝ lockval + (1 << 16)，相当于next++
"    stxr    %w2, %w1, %3\n"－－－－－－－－－－－－－－lock ＝ newval
"    cbnz    %w2, 1b\n"－－－－－－－－－－－－－－是否有其他PE的执行流插入？有的话，重来。
    /* Did we get the lock? */
"    eor    %w1, %w0, %w0, ror #16\n"－－lockval中的next域就是自己的号码牌，判断是否等于owner
"    cbz    %w1, 3f\n"－－－－－－－－－－－－－－－－如果等于，持锁进入临界区
    /*
     * No: spin on the owner. Send a local event to avoid missing an
     * unlock before the exclusive load.
     */
"    sevl\n"
"2:    wfe\n"－－－－－－－－－－－－－－－－－－－－否则进入spin
"    ldaxrh    %w2, %4\n"－－－－（A）－－－－－－－－－其他cpu唤醒本cpu，获取当前owner值
"    eor    %w1, %w2, %w0, lsr #16\n"－－－－－－－－－自己的号码牌是否等于owner？
"    cbnz    %w1, 2b\n"－－－－－－－－－－如果等于，持锁进入临界区，否者回到2，即继续spin
    /* We got the lock. Critical section starts here. */
"3:"
    : "=&r" (lockval), "=&r" (newval), "=&r" (tmp), "+Q" (*lock)
    : "Q" (lock->owner), "I" (1 << TICKET_SHIFT)
    : "memory");
}

===============================================================================================================================================================

#define ATOMIC_INIT(i)	{ (i) }											//include\asm-generic\atomic.h

#define	__ARCH_SPIN_LOCK_UNLOCKED	{ { .val = ATOMIC_INIT(0) } }		//include\asm-generic\qspinlock_types.h

#define __RAW_SPIN_LOCK_INITIALIZER(lockname)	\						//以下定义路径：include\linux\spinlock_types.h
	{					\
	.raw_lock = __ARCH_SPIN_LOCK_UNLOCKED,	\
	SPIN_DEBUG_INIT(lockname)		\									//debug，一般没有定义
	SPIN_DEP_MAP_INIT(lockname) }										//debug，一般没有定义

#define __SPIN_LOCK_INITIALIZER(lockname) \
	{ { .rlock = __RAW_SPIN_LOCK_INITIALIZER(lockname) } }
#define __SPIN_LOCK_UNLOCKED(lockname) \
	(spinlock_t ) __SPIN_LOCK_INITIALIZER(lockname)

#define DEFINE_SPINLOCK(x)	spinlock_t x = __SPIN_LOCK_UNLOCKED(x)

===============================================================================================================================================================

UP中的实现
static inline void __preempt_count_add(int val)							//arch\arm64\include\asm\preempt.h
{																		//读取当前线程的count值，加val，写入去
	u32 pc = READ_ONCE(current_thread_info()->preempt.count);
	pc += val;
	WRITE_ONCE(current_thread_info()->preempt.count, pc);
}

#define preempt_count_add(val)	__preempt_count_add(val)
#define preempt_count_inc() preempt_count_add(1)

#define preempt_disable() \												//include\linux\preempt.h
do { \
	preempt_count_inc(); \
	barrier(); \
} while (0)

#define __LOCK(lock) \
  do { preempt_disable(); ___LOCK(lock); } while (0)

#define _raw_spin_lock(lock)			__LOCK(lock)					//include\linux\spinlock_api_up.h

===============================================================================================================================================================

typedef struct {
	volatile int val;
} atomic_t;

typedef struct qspinlock {												//全局锁
	union {
		atomic_t val;

		/*
		 * By using the whole 2nd least significant byte for the
		 * pending bit, we can allow better optimization of the lock
		 * acquisition for the pending bit holder.
		 */
		struct {
			u8	locked;
			u8	pending;
		};
		struct {
			u16	locked_pending;
			u16	tail;
		};
	};
} arch_spinlock_t;

typedef struct raw_spinlock {											//以下定义路径：include\linux\spinlock_types.h
	arch_spinlock_t raw_lock;
} raw_spinlock_t;

typedef struct spinlock {
	union {
		struct raw_spinlock rlock;
	};
} spinlock_t;

===============================================================


SMP实现：

static inline void __preempt_count_add(int val)							//arch\arm64\include\asm\preempt.h
{
	u32 pc = READ_ONCE(current_thread_info()->preempt.count);			//current->preempt.count线程抢占应用计数器(32bit)，为0时表示当前线程可以被抢占，大于0时表示不能被抢占，是per-cpu变量
	pc += val;
	WRITE_ONCE(current_thread_info()->preempt.count, pc);
}

#define preempt_count_add(val)	__preempt_count_add(val)

#define preempt_count_inc() preempt_count_add(1)

#define preempt_disable() \												//include\linux\preempt.h
do { \
	preempt_count_inc(); \
	barrier(); \
} while (0)

==================================================================================================================



#define _Q_LOCKED_OFFSET	0
#define _Q_LOCKED_VAL		(1U << _Q_LOCKED_OFFSET)
static __always_inline void queued_spin_lock(struct qspinlock *lock)
{
	u32 val = 0;

	if (likely(atomic_try_cmpxchg_acquire(&lock->val, &val, _Q_LOCKED_VAL)))
		return;

	queued_spin_lock_slowpath(lock, val);
}

#define arch_spin_lock(l)		queued_spin_lock(l)						//include\asm-generic\qspinlock.h

下面__acquires(x) 和 __releases(x), __acquire(x) 和 __release(x) 必须配对使用, 否则 Sparse 会给出警，都是和今天代码检测相关的
# define __acquires(x)	__attribute__((context(x,0,1)))					//参数x在执行前引用计数必须是0，执行后引用计数必须为1
# define __acquire(x)	__context__(x,1)								//参数x在后面的运行过程中需要+1？

static inline void do_raw_spin_lock(raw_spinlock_t *lock) __acquires(lock)		//include\linux\spinlock.h
{
	__acquire(lock);
	arch_spin_lock(&lock->raw_lock);
	mmiowb_spin_lock();													//一般没有定义CONFIG_MMIOWB，函数为空
}

#define LOCK_CONTENDED(_lock, try, lock) \								//一般没有定义CONFIG_LOCK_STAT
	lock(_lock)	==> do_raw_spin_lock(_lock)

static inline void __raw_spin_lock(raw_spinlock_t *lock)				//include\linux\spinlock_api_smp.h
{
	preempt_disable();
	spin_acquire(&lock->dep_map, 0, 0, _RET_IP_);						//一般没有定义CONFIG_LOCKDEP则为空
	LOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);
}

void __lockfunc _raw_spin_lock(raw_spinlock_t *lock)					//kernel\locking\spinlock.c
{
	__raw_spin_lock(lock);
}

#define raw_spin_lock(lock)	_raw_spin_lock(lock)

static __always_inline void spin_lock(spinlock_t *lock)					//include\linux\spinlock.h
{
	raw_spin_lock(&lock->rlock);
}




用法：
static DEFINE_SPINLOCK(swapper_pgdir_lock);								//静态定义spinlock_t类型的结构体，初始化其值：swapper_pgdir_lock.rlock.raw_lock.val=0
spin_lock_init(x);														//动态初始化自旋锁


(1)void spin_lock(spinlock_t *lock);									//申请自旋锁，如果锁被其他处理器占有，当前处理器自旋等待。
(2)void spin_lock_bh(spinlock_t *lock);									//申请自旋锁，并且禁止当前处理器的软中断。
(3)void spin_lock_irq(spinlock_t *lock);								//申请自旋锁，并且禁止当前处理器的硬中断。
(4)spin_lock_irqsave(lock, flags);										//申请自旋锁，保存当前处理器的硬中断状态，并且禁止当前处理器的硬中断。
(5)int spin_trylock(spinlock_t *lock);									//申请自旋锁，如果申请成功，返回1；如果锁被其他处理器占有，当前处理器不等待，立即返回0。

...

(1)void spin_unlock(spinlock_t *lock);
(2)void spin_unlock_bh(spinlock_t *lock);								//释放自旋锁，并且开启当前处理器的软中断。
(3)void spin_unlock_irq(spinlock_t *lock);								//释放自旋锁，并且开启当前处理器的硬中断。
(4)void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags);	//释放自旋锁，并且恢复当前处理器的硬中断状态。







